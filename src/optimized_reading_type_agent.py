import os
import json
import csv
import datetime
import xml.etree.ElementTree as ET
from difflib import SequenceMatcher
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
import re
from typing import Dict, List, Optional, Tuple

from .enhanced_semantic_parser import EnhancedSemanticParser
from .enhanced_dictionary_manager import EnhancedDictionaryManager

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®DeepSeek APIå¯†é’¥
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

class OptimizedReadingTypeAgent:
    """ä¼˜åŒ–ç‰ˆReadingTypeæ™ºèƒ½ç¼–ç åŠ©æ‰‹"""
    
    def __init__(self):
        self.conversation_history = []
        
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ï¼Œé…ç½®DeepSeekåŸºç¡€URL
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.codes_file = "reading_type_codes.csv"
        self.dictionaries_file = "field_dictionaries.csv"
        self.history_file = "operation_history.csv"
        
        # åˆå§‹åŒ–å¢å¼ºç»„ä»¶
        self.dictionary_manager = EnhancedDictionaryManager(self.dictionaries_file)
        self.semantic_parser = EnhancedSemanticParser(self.dictionary_manager)
        
        # åŠ è½½ç¼–ç åº“
        self.reading_type_codes = self.load_reading_type_codes()
        
        # ReadingTypeå­—æ®µå®šä¹‰
        self.field_names = [
            "macroPeriod", "aggregate", "measurePeriod", "accumulationBehaviour",
            "flowDirection", "commodity", "measurementKind", "harmonic",
            "argumentNumerator", "TOU", "cpp", "tier", "phase", "multiplier", "uom", "currency"
        ]
        
        # è®¾ç½®å¯ç”¨å·¥å…·
        self.available_tools = {
            "search_reading_type": self.search_reading_type,
            "generate_reading_type_enhanced": self.generate_reading_type_enhanced,
            "query_dictionary_enhanced": self.query_dictionary_enhanced,
            "smart_dictionary_search": self.smart_dictionary_search,
            "validate_reading_type": self.validate_reading_type,
            "export_data": self.export_data,
            "view_codes_library": self.view_codes_library,
            "get_analysis_report": self.get_analysis_report
        }
        
        # ç”¨æˆ·åé¦ˆå­¦ä¹ 
        self.feedback_data = []
    
    def load_reading_type_codes(self):
        """åŠ è½½ReadingTypeç¼–ç åº“"""
        try:
            df = pd.read_csv(self.codes_file)
            return df.to_dict('records')
        except FileNotFoundError:
            print(f"è­¦å‘Š: ç¼–ç åº“æ–‡ä»¶ {self.codes_file} æœªæ‰¾åˆ°")
            return []
    
    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation_history.append({"role": role, "content": content})
    
    def search_reading_type(self, args):
        """å¢å¼ºç‰ˆç¼–ç æœç´¢"""
        search_term = args.get("name", "").strip()
        
        if not search_term:
            return "âŒ è¯·æä¾›è¦æœç´¢çš„é‡æµ‹åç§°"
        
        # ä½¿ç”¨å¢å¼ºçš„æœç´¢é€»è¾‘
        exact_matches = []
        fuzzy_matches = []
        
        # é¢„å¤„ç†æœç´¢è¯
        processed_term = self.semantic_parser._preprocess_text(search_term)
        
        for code in self.reading_type_codes:
            name = code.get('name', '')
            description = code.get('description', '')
            
            # ç²¾ç¡®åŒ¹é…
            if search_term.lower() == name.lower():
                exact_matches.append(code)
            else:
                # å¢å¼ºçš„æ¨¡ç³ŠåŒ¹é…
                score = self._calculate_search_score(processed_term, name, description)
                if score > 0.4:  # æé«˜é˜ˆå€¼
                    fuzzy_matches.append((code, score))
        
        # æ„å»ºè¿”å›ç»“æœ
        result = []
        
        if exact_matches:
            result.append("âœ… æ‰¾åˆ°ç²¾ç¡®åŒ¹é…:")
            for match in exact_matches:
                result.append(f"ğŸ“Š åç§°: {match.get('name', 'N/A')}")
                result.append(f"ğŸ”¢ ReadingTypeID: {match.get('reading_type_id', 'N/A')}")
                result.append(f"ğŸ“ è¯´æ˜: {match.get('description', 'N/A')}")
                result.append(f"ğŸ·ï¸ ç±»åˆ«: {match.get('category', 'N/A')}")
                result.append("---")
        
        if fuzzy_matches and not exact_matches:
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            fuzzy_matches.sort(key=lambda x: x[1], reverse=True)
            result.append("ğŸ” æ‰¾åˆ°ç›¸ä¼¼çš„ç¼–ç :")
            for i, (match, similarity) in enumerate(fuzzy_matches[:5], 1):
                result.append(f"{i}. {match.get('name', 'N/A')} (ç›¸ä¼¼åº¦: {similarity:.1%})")
                result.append(f"   ReadingTypeID: {match.get('reading_type_id', 'N/A')}")
                result.append(f"   è¯´æ˜: {match.get('description', 'N/A')[:80]}...")
        
        if not exact_matches and not fuzzy_matches:
            result.append(f"âŒ æœªæ‰¾åˆ°ä¸'{search_term}'ç›¸å…³çš„ç¼–ç ")
            result.append("ğŸ’¡ æˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆæ–°çš„ReadingTypeID")
            # è‡ªåŠ¨åˆ†æå¹¶æä¾›å»ºè®®
            analysis, confidence = self.semantic_parser.analyze_description_enhanced(search_term)
            if confidence > 0.3:
                result.append("ğŸ¤– åŸºäºæè¿°çš„åˆæ­¥åˆ†æ:")
                result.append(f"   ç½®ä¿¡åº¦: {confidence:.1%}")
                result.append("   å»ºè®®ä½¿ç”¨ 'ç”Ÿæˆç¼–ç ' åŠŸèƒ½è·å–è¯¦ç»†ç»“æœ")
        
        # è®°å½•æ“ä½œå†å²
        self.log_operation(search_term, "search", "\n".join(result))
        
        return "\n".join(result)
    
    def _calculate_search_score(self, search_term: str, name: str, description: str) -> float:
        """è®¡ç®—æœç´¢ç›¸å…³æ€§åˆ†æ•°"""
        scores = []
        
        name_lower = name.lower()
        desc_lower = description.lower()
        
        # 1. ç›´æ¥åŒ…å«åŒ¹é…
        if search_term in name_lower:
            scores.append(0.8)
        if search_term in desc_lower:
            scores.append(0.6)
        
        # 2. å…³é”®è¯åŒ¹é…
        search_words = search_term.split()
        name_words = name_lower.split()
        desc_words = desc_lower.split()
        
        for word in search_words:
            if len(word) >= 2:
                if word in name_words:
                    scores.append(0.7)
                if word in desc_words:
                    scores.append(0.5)
        
        # 3. åºåˆ—åŒ¹é…
        name_similarity = SequenceMatcher(None, search_term, name_lower).ratio()
        if name_similarity > 0.5:
            scores.append(name_similarity * 0.6)
        
        desc_similarity = SequenceMatcher(None, search_term, desc_lower).ratio()
        if desc_similarity > 0.3:
            scores.append(desc_similarity * 0.4)
        
        return max(scores) if scores else 0.0
    
    def generate_reading_type_enhanced(self, args):
        """å¢å¼ºç‰ˆReadingTypeç”Ÿæˆ"""
        description = args.get("description", "")
        field_values = args.get("field_values", {})
        
        if not description and not field_values:
            return "âŒ è¯·æä¾›é‡æµ‹æè¿°æˆ–å­—æ®µå€¼"
        
        try:
            if field_values:
                # ç›´æ¥ä½¿ç”¨æä¾›çš„å­—æ®µå€¼
                analysis = field_values
                confidence = 0.9  # ç”¨æˆ·ç›´æ¥æä¾›çš„å€¼ç»™é«˜ç½®ä¿¡åº¦
            else:
                # ä½¿ç”¨å¢å¼ºçš„è¯­ä¹‰åˆ†æ
                analysis, confidence = self.semantic_parser.analyze_description_enhanced(description)
            
            # éªŒè¯å­—æ®µç»„åˆ
            field_dict = {}
            for i, field_name in enumerate(self.field_names):
                field_key = f"field_{i+1}"
                if field_key in analysis:
                    field_dict[field_name] = str(analysis[field_key])
            
            is_valid, warnings = self.dictionary_manager.validate_field_combination(field_dict)
            
            # æ„å»ºReadingTypeID
            reading_type_id = self.semantic_parser.build_reading_type_id(analysis)
            
            result = []
            result.append("ğŸ¤– å¢å¼ºç‰ˆAIåˆ†æç»“æœ:")
            result.append(f"ğŸ”¢ ç”Ÿæˆçš„ReadingTypeID: {reading_type_id}")
            result.append(f"ğŸ¯ åˆ†æç½®ä¿¡åº¦: {confidence:.1%}")
            result.append("")
            
            # æ˜¾ç¤ºå­—æ®µè¯¦æƒ…
            result.append("ğŸ“‹ å­—æ®µè§£æè¯¦æƒ…:")
            for i, field_name in enumerate(self.field_names):
                field_key = f"field_{i+1}"
                value = analysis.get(field_key, 0)
                if value != 0:
                    chinese_name = self.dictionary_manager.chinese_field_names.get(field_name, field_name)
                    field_desc = self.dictionary_manager.get_field_description(field_name, str(value))
                    result.append(f"  {chinese_name}: {value} ({field_desc})")
            
            # æ˜¾ç¤ºéªŒè¯è­¦å‘Š
            if warnings:
                result.append("")
                result.append("âš ï¸ å­—æ®µç»„åˆå»ºè®®:")
                for warning in warnings:
                    result.append(f"  â€¢ {warning}")
            
            # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
            suggestions = self.semantic_parser.suggest_alternatives(analysis, confidence)
            if suggestions:
                result.append("")
                result.extend(suggestions)
            
            result.append("")
            result.append("âœ… æ˜¯å¦é‡‡çº³æ­¤ç¼–ç ï¼Ÿè¾“å…¥'æ˜¯'ç¡®è®¤ï¼Œ'å¦'å–æ¶ˆï¼Œæˆ–æå‡ºä¿®æ”¹å»ºè®®ã€‚")
            
            # è®°å½•æ“ä½œå†å²
            self.log_operation(description, "generate_enhanced", "\n".join(result))
            
            return "\n".join(result)
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆç¼–ç æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def query_dictionary_enhanced(self, args):
        """å¢å¼ºç‰ˆå­—å…¸æŸ¥è¯¢"""
        field_name = args.get("field_name", "").strip()
        search_term = args.get("search_term", "").strip()
        
        if not field_name and not search_term:
            # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µæ¦‚è§ˆ
            result = ["ğŸ“š ReadingTypeå­—æ®µå­—å…¸ (å¢å¼ºç‰ˆ):"]
            stats = self.dictionary_manager.get_field_usage_statistics()
            
            for i, (name, chinese_name) in enumerate(self.dictionary_manager.chinese_field_names.items(), 1):
                field_stats = stats.get(name, {})
                total = field_stats.get('total_values', 0)
                result.append(f"{i:2d}. {name} ({chinese_name}) - {total}ä¸ªå€¼")
            
            result.append("\nğŸ’¡ ä½¿ç”¨æ–¹å¼:")
            result.append("   â€¢ 'æŸ¥è¯¢å­—å…¸ [å­—æ®µå]' - æŸ¥çœ‹å­—æ®µæ‰€æœ‰å€¼")
            result.append("   â€¢ 'æ™ºèƒ½æœç´¢ [å…³é”®è¯]' - è·¨å­—æ®µæ™ºèƒ½æœç´¢")
            return "\n".join(result)
        
        if field_name and not search_term:
            # æŸ¥è¯¢ç‰¹å®šå­—æ®µ
            if field_name not in self.dictionary_manager.field_dictionaries:
                # å°è¯•å­—æ®µåå»ºè®®
                suggestions = self.dictionary_manager.get_field_suggestions(field_name)
                if suggestions:
                    result = [f"âŒ å­—æ®µ '{field_name}' ä¸å­˜åœ¨ï¼Œæ‚¨æ˜¯å¦è¦æŸ¥è¯¢:"]
                    for eng_name, chi_name in suggestions:
                        result.append(f"   â€¢ {eng_name} ({chi_name})")
                    return "\n".join(result)
                else:
                    return f"âŒ æœªæ‰¾åˆ°å­—æ®µ '{field_name}'"
            
            # è·å–å­—æ®µè¯¦ç»†ä¿¡æ¯
            items = self.dictionary_manager.get_field_options(field_name, limit=30)
            chinese_name = self.dictionary_manager.chinese_field_names.get(field_name, field_name)
            
            result = [f"ğŸ“– å­—æ®µ '{field_name}' ({chinese_name}) è¯¦æƒ…:"]
            result.append(f"ğŸ“Š æ€»è®¡ {len(self.dictionary_manager.field_dictionaries[field_name])} ä¸ªå€¼")
            result.append("")
            
            for item in items:
                marker = "ğŸ”§" if item.get('is_custom', False) else "ğŸ“Œ"
                result.append(f"{marker} {item['value']}: {item['display_name']}")
                if item.get('description'):
                    # æˆªæ–­é•¿æè¿°
                    desc = str(item['description'])[:80]
                    if len(str(item['description'])) > 80:
                        desc += "..."
                    result.append(f"    ğŸ“ {desc}")
            
            if len(self.dictionary_manager.field_dictionaries[field_name]) > 30:
                remaining = len(self.dictionary_manager.field_dictionaries[field_name]) - 30
                result.append(f"\n... è¿˜æœ‰ {remaining} ä¸ªå€¼ï¼Œä½¿ç”¨æœç´¢åŠŸèƒ½æŸ¥æ‰¾ç‰¹å®šå€¼")
            
            return "\n".join(result)
        
        # å¦‚æœæœ‰æœç´¢è¯ï¼Œä½¿ç”¨æ™ºèƒ½æœç´¢
        return self.smart_dictionary_search({"search_term": search_term, "field_name": field_name})
    
    def smart_dictionary_search(self, args):
        """æ™ºèƒ½å­—å…¸æœç´¢"""
        search_term = args.get("search_term", "").strip()
        field_name = args.get("field_name", "").strip()
        
        if not search_term:
            return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
        
        # ä½¿ç”¨å¢å¼ºçš„æœç´¢åŠŸèƒ½
        results = self.dictionary_manager.smart_search(
            search_term=search_term,
            field_name=field_name,
            max_results=15,
            threshold=0.3
        )
        
        if not results:
            return f"âŒ æœªæ‰¾åˆ°ä¸ '{search_term}' ç›¸å…³çš„å­—å…¸é¡¹"
        
        result = [f"ğŸ” æ™ºèƒ½æœç´¢ç»“æœ (å…³é”®è¯: '{search_term}'):"]
        if field_name:
            result[0] += f" é™å®šå­—æ®µ: {field_name}"
        result.append("")
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„æ˜¾ç¤º
        high_relevance = [r for r in results if r[2] >= 0.8]
        medium_relevance = [r for r in results if 0.5 <= r[2] < 0.8]
        low_relevance = [r for r in results if r[2] < 0.5]
        
        if high_relevance:
            result.append("ğŸ¯ é«˜ç›¸å…³åº¦åŒ¹é…:")
            for fname, item, score in high_relevance:
                chinese_name = self.dictionary_manager.chinese_field_names.get(fname, fname)
                result.append(f"  ğŸ“Œ {fname}({chinese_name}).{item['value']}: {item['display_name']}")
                result.append(f"      ç›¸ä¼¼åº¦: {score:.1%}")
        
        if medium_relevance:
            result.append("\nğŸ”¸ ä¸­ç­‰ç›¸å…³åº¦åŒ¹é…:")
            for fname, item, score in medium_relevance[:5]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                chinese_name = self.dictionary_manager.chinese_field_names.get(fname, fname)
                result.append(f"  ğŸ“‹ {fname}({chinese_name}).{item['value']}: {item['display_name']}")
        
        if low_relevance:
            result.append(f"\nğŸ”¹ å…¶ä»–å¯èƒ½ç›¸å…³çš„ç»“æœ: {len(low_relevance)} ä¸ª")
        
        return "\n".join(result)
    
    def validate_reading_type(self, args):
        """éªŒè¯ReadingTypeç¼–ç """
        reading_type_id = args.get("reading_type_id", "").strip()
        
        if not reading_type_id:
            return "âŒ è¯·æä¾›ReadingTypeID"
        
        try:
            # è§£æReadingTypeID
            field_values = self.semantic_parser.parse_reading_type_id(reading_type_id)
            
            # è½¬æ¢ä¸ºå­—æ®µåæ ¼å¼
            field_dict = {}
            for i, field_name in enumerate(self.field_names):
                field_key = f"field_{i+1}"
                if field_key in field_values:
                    field_dict[field_name] = str(field_values[field_key])
            
            # éªŒè¯å­—æ®µç»„åˆ
            is_valid, warnings = self.dictionary_manager.validate_field_combination(field_dict)
            
            result = []
            result.append("ğŸ” ReadingTypeç¼–ç éªŒè¯ç»“æœ:")
            result.append(f"ğŸ”¢ ç¼–ç : {reading_type_id}")
            result.append(f"âœ… åŸºæœ¬æ ¼å¼: {'æœ‰æ•ˆ' if len(reading_type_id.split('-')) == 16 else 'æ— æ•ˆ'}")
            result.append("")
            
            # æ˜¾ç¤ºå­—æ®µè§£æ
            result.append("ğŸ“‹ å­—æ®µè§£æ:")
            for field_name, value in field_dict.items():
                if value != '0':
                    chinese_name = self.dictionary_manager.chinese_field_names.get(field_name, field_name)
                    field_desc = self.dictionary_manager.get_field_description(field_name, value)
                    
                    # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
                    context = self.dictionary_manager.get_value_context(field_name, value)
                    status = "âœ…" if context else "âš ï¸"
                    
                    result.append(f"  {status} {chinese_name}: {value} ({field_desc})")
            
            # æ˜¾ç¤ºéªŒè¯è­¦å‘Š
            if warnings:
                result.append("")
                result.append("âš ï¸ å­—æ®µç»„åˆå»ºè®®:")
                for warning in warnings:
                    result.append(f"  â€¢ {warning}")
            else:
                result.append("\nâœ… å­—æ®µç»„åˆéªŒè¯é€šè¿‡")
            
            return "\n".join(result)
            
        except Exception as e:
            return f"âŒ éªŒè¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def get_analysis_report(self, args):
        """è·å–è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        description = args.get("description", "")
        
        if not description:
            return "âŒ è¯·æä¾›è¦åˆ†æçš„æè¿°"
        
        try:
            # è¿›è¡Œå¢å¼ºåˆ†æ
            analysis, confidence = self.semantic_parser.analyze_description_enhanced(description)
            
            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            report = self.semantic_parser.export_analysis_report(description, analysis, confidence)
            
            return report
            
        except Exception as e:
            return f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def view_codes_library(self, args):
        """æŸ¥çœ‹ç¼–ç åº“ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        page = int(args.get("page", 1))
        per_page = int(args.get("per_page", 20))
        category = args.get("category", "")
        
        # ç­›é€‰æ•°æ®
        filtered_codes = self.reading_type_codes
        if category:
            filtered_codes = [code for code in filtered_codes 
                            if code.get('category', '').lower() == category.lower()]
        
        total = len(filtered_codes)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        page_codes = filtered_codes[start_idx:end_idx]
        
        result = [f"ğŸ“š ç¼–ç åº“ (ç¬¬{page}é¡µ, å…±{total}æ¡è®°å½•)"]
        if category:
            result[0] += f" - ç±»åˆ«: {category}"
        
        result.append("=" * 50)
        
        for i, code in enumerate(page_codes, start_idx + 1):
            result.append(f"{i:3d}. {code.get('name', 'N/A')}")
            result.append(f"     ID: {code.get('reading_type_id', 'N/A')}")
            result.append(f"     æè¿°: {code.get('description', 'N/A')[:60]}...")
            result.append("")
        
        # åˆ†é¡µä¿¡æ¯
        total_pages = (total + per_page - 1) // per_page
        if total_pages > 1:
            result.append(f"ğŸ“„ ç¬¬ {page}/{total_pages} é¡µ")
            if page < total_pages:
                result.append("ğŸ’¡ ä½¿ç”¨ 'æŸ¥çœ‹ç¼–ç åº“ é¡µç =N' æŸ¥çœ‹å…¶ä»–é¡µé¢")
        
        return "\n".join(result)
    
    def export_data(self, args):
        """å¯¼å‡ºæ•°æ®ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        data_type = args.get("type", "codes")
        
        if data_type == "dictionary":
            field_name = args.get("field_name", "")
            return self.dictionary_manager.export_enhanced_report(field_name)
        else:
            # å¯¼å‡ºç¼–ç åº“
            result = []
            result.append("ğŸ“Š ReadingTypeç¼–ç åº“å¯¼å‡ºæŠ¥å‘Š")
            result.append("=" * 50)
            result.append(f"ğŸ“… å¯¼å‡ºæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            result.append(f"ğŸ“ˆ æ€»ç¼–ç æ•°é‡: {len(self.reading_type_codes)}")
            result.append("")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            categories = {}
            for code in self.reading_type_codes:
                cat = code.get('category', 'æœªåˆ†ç±»')
                categories[cat] = categories.get(cat, 0) + 1
            
            result.append("ğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
            for cat, count in sorted(categories.items()):
                result.append(f"  {cat}: {count} ä¸ª")
            
            return "\n".join(result)
    
    def log_operation(self, input_text, operation_type, result, user_action="pending"):
        """è®°å½•æ“ä½œå†å²"""
        try:
            log_entry = {
                'timestamp': datetime.datetime.now().isoformat(),
                'input': input_text,
                'operation': operation_type,
                'result_length': len(result),
                'user_action': user_action
            }
            
            # ç®€å•çš„CSVè®°å½•
            with open(self.history_file, 'a', newline='', encoding='utf-8') as f:
                if f.tell() == 0:  # æ–‡ä»¶ä¸ºç©ºï¼Œå†™å…¥æ ‡é¢˜
                    f.write('timestamp,input,operation,result_length,user_action\n')
                f.write(f"{log_entry['timestamp']},{log_entry['input']},{log_entry['operation']},{log_entry['result_length']},{log_entry['user_action']}\n")
        except Exception as e:
            print(f"è®°å½•å†å²å¤±è´¥: {e}")
    
    def add_user_feedback(self, description: str, generated_code: str, 
                         user_rating: int, correct_code: str = ""):
        """æ·»åŠ ç”¨æˆ·åé¦ˆä»¥æ”¹è¿›ç³»ç»Ÿ"""
        feedback = {
            'timestamp': datetime.datetime.now().isoformat(),
            'description': description,
            'generated_code': generated_code,
            'user_rating': user_rating,  # 1-5 åˆ†
            'correct_code': correct_code
        }
        self.feedback_data.append(feedback)
        
        # å¦‚æœè¯„åˆ†è¾ƒä½ï¼Œå¯ä»¥ç”¨äºæ”¹è¿›å…³é”®è¯æ˜ å°„
        if user_rating <= 2 and correct_code:
            self._learn_from_feedback(description, correct_code)
    
    def _learn_from_feedback(self, description: str, correct_code: str):
        """ä»ç”¨æˆ·åé¦ˆä¸­å­¦ä¹ ï¼ˆç®€å•çš„è§„åˆ™æ›´æ–°ï¼‰"""
        try:
            # è§£ææ­£ç¡®çš„ç¼–ç 
            correct_analysis = self.semantic_parser.parse_reading_type_id(correct_code)
            
            # åˆ†ææè¿°ä¸­çš„å…³é”®è¯
            keywords = self.semantic_parser._preprocess_text(description).split()
            
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å­¦ä¹ é€»è¾‘
            # ä¾‹å¦‚æ›´æ–°å…³é”®è¯æ˜ å°„æƒé‡ç­‰
            print(f"å­¦ä¹ åé¦ˆ: {description} -> {correct_code}")
            
        except Exception as e:
            print(f"å­¦ä¹ åé¦ˆå¤±è´¥: {e}")
    
    # ä¿æŒä¸åŸç‰ˆæœ¬çš„å…¼å®¹æ€§
    def handle_function_call(self, function_call):
        """å¤„ç†å‡½æ•°è°ƒç”¨"""
        function_name = function_call.get("name")
        function_args = {}
        
        if function_call.get("arguments"):
            try:
                function_args = json.loads(function_call.get("arguments", "{}"))
            except:
                pass
        
        if function_name in self.available_tools:
            function_response = self.available_tools[function_name](function_args)
            
            # æ·»åŠ å‡½æ•°ç»“æœåˆ°å¯¹è¯å†å²
            self.add_message("function", {
                "name": function_name,
                "content": function_response
            })
            
            return function_response
        else:
            return f"é”™è¯¯: æœªçŸ¥çš„å‡½æ•° '{function_name}'"
    
    def get_response(self, user_input, stream=True):
        """è·å–AIå›å¤ï¼ˆä¿æŒä¸åŸç‰ˆæœ¬å…¼å®¹ï¼‰"""
        # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
        self.add_message("user", user_input)
        
        try:
            # å·¥å…·æè¿°ï¼ˆæ›´æ–°ç‰ˆæœ¬ï¼‰
            tools = [
                {
                    "type": "function",
                    "function": {
                        "name": "search_reading_type",
                        "description": "æœç´¢ç°æœ‰çš„ReadingTypeç¼–ç ",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string", "description": "è¦æœç´¢çš„é‡æµ‹åç§°"}
                            },
                            "required": ["name"]
                        }
                    }
                },
                {
                    "type": "function",
                    "function": {
                        "name": "generate_reading_type_enhanced",
                        "description": "ä½¿ç”¨å¢å¼ºAIç®—æ³•ç”ŸæˆReadingTypeç¼–ç ",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "description": {"type": "string", "description": "é‡æµ‹æè¿°"},
                                "field_values": {"type": "object", "description": "æŒ‡å®šçš„å­—æ®µå€¼"}
                            }
                        }
                    }
                },
                {
                    "type": "function",
                    "function": {
                        "name": "smart_dictionary_search",
                        "description": "æ™ºèƒ½å­—å…¸æœç´¢åŠŸèƒ½",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "search_term": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                                "field_name": {"type": "string", "description": "é™å®šå­—æ®µå(å¯é€‰)"}
                            },
                            "required": ["search_term"]
                        }
                    }
                },
                {
                    "type": "function",
                    "function": {
                        "name": "validate_reading_type",
                        "description": "éªŒè¯ReadingTypeç¼–ç çš„æœ‰æ•ˆæ€§",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "reading_type_id": {"type": "string", "description": "è¦éªŒè¯çš„ReadingTypeID"}
                            },
                            "required": ["reading_type_id"]
                        }
                    }
                }
            ]
            
            # è°ƒç”¨DeepSeek API
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=self.conversation_history,
                tools=tools,
                tool_choice="auto",
                stream=False
            )
            
            response_message = response.choices[0].message
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
                self.add_message("assistant", response_message)
                
                # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
                for tool_call in response_message.tool_calls:
                    function_info = {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments
                    }
                    tool_result = self.handle_function_call(function_info)
                    print(f"ğŸ”§ ä½¿ç”¨å·¥å…·: {tool_call.function.name}")
                
                # å†æ¬¡è°ƒç”¨APIè·å–æœ€ç»ˆå›å¤
                if stream:
                    print("\nğŸ¤– AIåŠ©æ‰‹: ", end="", flush=True)
                    second_response = self.client.chat.completions.create(
                        model="deepseek-chat",
                        messages=self.conversation_history,
                        stream=True
                    )
                    
                    full_response = ""
                    for chunk in second_response:
                        if chunk.choices and len(chunk.choices) > 0:
                            content = chunk.choices[0].delta.content
                            if content:
                                print(content, end="", flush=True)
                                full_response += content
                    
                    print()
                    ai_response = full_response
                else:
                    second_response = self.client.chat.completions.create(
                        model="deepseek-chat",
                        messages=self.conversation_history,
                        stream=False
                    )
                    ai_response = second_response.choices[0].message.content
                    print(f"\nğŸ¤– AIåŠ©æ‰‹: {ai_response}")
                
                self.add_message("assistant", ai_response)
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å›å¤
                ai_response = response_message.content
                print(f"\nğŸ¤– AIåŠ©æ‰‹: {ai_response}")
                self.add_message("assistant", ai_response)
            
            return ai_response
            
        except Exception as e:
            error_msg = f"âŒ è·å–å›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(error_msg)
            return error_msg
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = [] 